from keras.models import Sequential, Model
from keras.layers import UpSampling2D, Conv2D, Activation, BatchNormalization, Reshape, Dense, Input, LeakyReLU, Dropout, Flatten, ZeroPadding2D
from keras.optimizers import Adam
from PIL import Image
import numpy as np
import os
import imageio

img_size = (224, 224)
upsample_layers = 5
starting_filters = 64
kernel_size = 3
channels = 3
image_path = 'patterns'
output_directory = 'my_art'
pictures_amount = 5 
batch_size = 10 
epochs = 15
optimizer = Adam(0.0002, 0.5)

if not os.path.exists(output_directory):
    os.makedirs(output_directory)
    
def build_discriminator():

    img_shape = (img_size[0], img_size[1], channels)

    model = Sequential()

    model.add(Conv2D(32, kernel_size=kernel_size, strides=2, input_shape=img_shape, padding="same"))  # 192x256 -> 96x128
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.25))

    model.add(Conv2D(64, kernel_size=kernel_size, strides=2, padding="same"))  # 96x128 -> 48x64
    model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.25))
    model.add(BatchNormalization(momentum=0.8))

    model.add(Conv2D(128, kernel_size=kernel_size, strides=2, padding="same"))  # 48x64 -> 24x32
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.25))
    model.add(BatchNormalization(momentum=0.8))

    model.add(Conv2D(256, kernel_size=kernel_size, strides=1, padding="same"))  # 24x32 -> 12x16
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.25))

    model.add(Conv2D(512, kernel_size=kernel_size, strides=1, padding="same"))  # 12x16 -> 6x8
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.25))

    model.add(Flatten())
    model.add(Dense(1, activation='sigmoid'))

    img = Input(shape=img_shape)
    validity = model(img)

    return Model(img, validity)

def build_generator():
    noise_shape = (100,)
    model = Sequential()
    model.add(Dense(starting_filters * (img_size[0] // (2 ** upsample_layers))  *  (img_size[1] // (2 ** upsample_layers)),
                    activation="relu", input_shape=noise_shape))
    model.add(Reshape(((img_size[0] // (2 ** upsample_layers)),
                        (img_size[1] // (2 ** upsample_layers)),
                        starting_filters)))
    model.add(BatchNormalization(momentum=0.8))
    model.add(UpSampling2D())  # 6x8 -> 12x16
    model.add(Conv2D(1024, kernel_size=kernel_size, padding="same"))
    model.add(Activation("relu"))
    model.add(BatchNormalization(momentum=0.8))
    model.add(UpSampling2D())  # 12x16 -> 24x32
    model.add(Conv2D(512, kernel_size=kernel_size, padding="same"))
    model.add(Activation("relu"))
    model.add(BatchNormalization(momentum=0.8))
    model.add(UpSampling2D())  # 24x32 -> 48x64
    model.add(Conv2D(256, kernel_size=kernel_size, padding="same"))
    model.add(Activation("relu"))
    model.add(BatchNormalization(momentum=0.8))
    model.add(UpSampling2D())  # 48x64 -> 96x128
    model.add(Conv2D(128, kernel_size=kernel_size, padding="same"))
    model.add(Activation("relu"))
    model.add(BatchNormalization(momentum=0.8))
    model.add(UpSampling2D())  # 96x128 -> 192x256
    model.add(Conv2D(64, kernel_size=kernel_size, padding="same"))
    model.add(Activation("relu"))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Conv2D(32, kernel_size=kernel_size, padding="same"))
    model.add(Activation("relu"))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Conv2D(channels, kernel_size=kernel_size, padding="same"))
    model.add(Activation("tanh"))
    noise = Input(shape=noise_shape)
    img = model(noise)

    return Model(noise, img)

def load_imgs(image_path):
    X_train = []
    files = os.listdir(image_path)
    for file in files:
        filename = image_path + '/' + file
        img = Image.open(filename)
        img = np.asarray(img)
        X_train.append(img)
    return np.asarray(X_train)

def save_imgs(epoch):
    noise = np.random.normal(0, 1, (pictures_amount, 100))
    imgs = generator.predict(noise)
    imgs = 0.5 * imgs + 0.5
    for i, img_array in enumerate(imgs):
        imageio.imwrite(output_directory + f"/{epoch+1}_{i+1}.png", img_array)
        

X_train = load_imgs(image_path)
X_train = (X_train.astype(np.float32) - 900) / 900
half_batch = batch_size // 2
for epoch in range(epochs):
    noise = np.random.normal(0, 1, (batch_size, 100)) 
    g_loss = combined.train_on_batch(noise, np.ones((batch_size, 1)))
    noise = np.random.normal(0, 1, (half_batch, 100))
    gen_imgs = generator.predict(noise)
    idx = np.random.randint(0, X_train.shape[0], half_batch)
    imgs = X_train[idx]
    d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))
    d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))
    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
    print(f"{epoch} [D loss: {d_loss[0]} | D Accuracy: {100 * d_loss[1]}] [G loss: {g_loss}]")
    save_imgs(epoch)
